{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 2.6: Leakage-Controlled Evaluation\n",
                "\n",
                "**Purpose**: Run inference on original vs sanitized text to measure actual F1 delta.\n",
                "\n",
                "**Requires**: GPU runtime (T4 recommended)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Clone Repository\n",
                "!git clone https://github.com/AngadSingh22/Text2Diag.git\n",
                "%cd Text2Diag"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Install Dependencies\n",
                "!pip install -q torch transformers accelerate scikit-learn datasets pyyaml"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Download Raw Dataset (REQUIRED before building canonical)\n",
                "!python scripts/inspect_raw_datasets.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Build Canonical Dataset\n",
                "!python scripts/02_build_reddit_canonical.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Train Model (or upload existing checkpoint)\n",
                "import os\n",
                "checkpoint_path = \"results/week2/checkpoints/checkpoint-4332\"\n",
                "\n",
                "# Always retrain since we don't have checkpoint\n",
                "print(\"Training model...\")\n",
                "!python scripts/03_train_baseline.py \\\n",
                "    --data_dir data/processed/reddit_mh_windows \\\n",
                "    --out_dir results/week2 \\\n",
                "    --model_name distilbert-base-uncased \\\n",
                "    --max_len 256 \\\n",
                "    --batch_size 8 \\\n",
                "    --grad_accum 4 \\\n",
                "    --epochs 3 \\\n",
                "    --lr 2e-5\n",
                "\n",
                "# Find the checkpoint\n",
                "import glob\n",
                "checkpoints = glob.glob(\"results/week2/checkpoints/checkpoint-*\")\n",
                "if checkpoints:\n",
                "    checkpoint_path = max(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
                "    print(f\"Using checkpoint: {checkpoint_path}\")\n",
                "else:\n",
                "    print(\"ERROR: No checkpoint found!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Run Leakage-Controlled Evaluation\n",
                "!python scripts/09_eval_sanitized.py \\\n",
                "    --checkpoint {checkpoint_path} \\\n",
                "    --data_dir data/processed/reddit_mh_windows \\\n",
                "    --out_dir results/week2/remediation \\\n",
                "    --sanitize_config configs/sanitize.yaml \\\n",
                "    --batch_size 32"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Check Results\n",
                "!cat results/week2/remediation/leakage_eval_metrics.md"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. View JSON Metrics\n",
                "import json\n",
                "with open('results/week2/remediation/leakage_eval_metrics.json', 'r') as f:\n",
                "    metrics = json.load(f)\n",
                "print(json.dumps(metrics, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9. Zip and Download Results\n",
                "!zip -r w26_results.zip results/week2/remediation\n",
                "from google.colab import files\n",
                "files.download('w26_results.zip')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}