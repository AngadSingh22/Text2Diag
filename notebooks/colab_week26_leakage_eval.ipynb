{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 2.6: Leakage-Controlled Evaluation\n",
                "\n",
                "**Purpose**: Run inference on original vs sanitized text to measure actual F1 delta.\n",
                "\n",
                "**Requires**: GPU runtime (T4 recommended)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Clone Repository\n",
                "!git clone https://github.com/AngadSingh22/Text2Diag.git\n",
                "%cd Text2Diag"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Install Dependencies\n",
                "!pip install -q torch transformers accelerate scikit-learn datasets pyyaml"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Regenerate Dataset (needed for inference)\n",
                "!python scripts/02_build_reddit_canonical.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Upload Week 2 Checkpoint (or use existing if you have it)\n",
                "# Option A: If you have the checkpoint locally, upload it\n",
                "# Option B: Retrain (takes ~20 min)\n",
                "\n",
                "import os\n",
                "checkpoint_path = \"results_week2/results/week2/checkpoints/checkpoint-4332\"\n",
                "\n",
                "if not os.path.exists(checkpoint_path):\n",
                "    print(\"Checkpoint not found. Retraining...\")\n",
                "    !python scripts/03_train_baseline.py \\\n",
                "        --data_dir data/processed/reddit_mh_windows \\\n",
                "        --out_dir results_week2/results/week2 \\\n",
                "        --model_name distilbert-base-uncased \\\n",
                "        --max_len 256 \\\n",
                "        --batch_size 8 \\\n",
                "        --grad_accum 4 \\\n",
                "        --epochs 3 \\\n",
                "        --lr 2e-5\n",
                "    checkpoint_path = \"results_week2/results/week2/checkpoints/checkpoint-4332\"\n",
                "else:\n",
                "    print(f\"Using existing checkpoint: {checkpoint_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Run Leakage-Controlled Evaluation\n",
                "!python scripts/09_eval_sanitized.py \\\n",
                "    --checkpoint $checkpoint_path \\\n",
                "    --data_dir data/processed/reddit_mh_windows \\\n",
                "    --out_dir results/week2/remediation \\\n",
                "    --sanitize_config configs/sanitize.yaml \\\n",
                "    --batch_size 32"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Check Results\n",
                "!cat results/week2/remediation/leakage_eval_metrics.md"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. View JSON Metrics\n",
                "import json\n",
                "with open('results/week2/remediation/leakage_eval_metrics.json', 'r') as f:\n",
                "    metrics = json.load(f)\n",
                "print(json.dumps(metrics, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Zip and Download Results\n",
                "!zip -r w26_results.zip results/week2/remediation\n",
                "from google.colab import files\n",
                "files.download('w26_results.zip')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}